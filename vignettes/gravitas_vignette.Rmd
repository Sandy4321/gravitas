---
title: "Exploring probability distributions for bivariate temporal granularities"
author: "<i>Sayani Gupta</i <br> <hr>"
output: 
  rmarkdown::html_vignette:
vignette: >
  %\VignetteIndexEntry{Exploring probability distributions for bivariate temporal granularities with gravitas}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
<!-- #Example vignette: 
https://github.com/njtierney/naniar/blob/master/vignettes/getting-started-w-naniar.Rmd -->
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  collapse = TRUE,
  comment = "#>",
  fig.height = 5,
  fig.width = 8,
  fig.align = "center",
  cache = FALSE
)
library(gravitas)
library(dplyr)
library(ggplot2)
library(tsibble)
library(lvplot)
```


# Introduction

Temporal data are available at various resolution depending on context. Often in time series analysis, data collected at finer scale needs to be explored also at coarser temporal scales. This approach requires deconstructing time in various possible ways leading to linear (e.g. days, weeks), circular (e.g. day-of-week) or aperiodic (e.g. day-of-month) time granularities. The hierarchical structure of time also creates a natural nested ordering resulting in single or multiple order-up granularities. For a conventional Gregorian calendar, examples of single-order-up granularity may include hour-of-day, minute-of-hour and examples of multiple-order-up granularity may include hour-of-week or minute-of-day. All of these granularites might be useful in throwing light on the periodicity of the data depending on the context.  

Exploration of large quantities of temporal data across so many different deconstructions of time becomes clumsy without a systematic approach. 

In this vignette, we describe the tools in the package `gravitas` for systematically exploring large quantities of temporal data across different time deconstructions by visualizing the probability distributions using range of graphics in `ggplot2` by making allowances for the following aspects of time granularities:  

## Computation

### Get set of possible granularities with `search_gran`

The `smart_meter10` data in the package is a [tsibble](https://tsibble.tidyverts.org/) providing the energy consumption for ten households from [customer trials](https://data.gov.au/dataset/ds-dga-4e21dea3-9b87-4610-94c7-15a8a77907ef/details?q=smart-meter). 

```{r data_look}
smart_meter10 %>% select(
  customer_id,
  reading_datetime,
  general_supply_kwh,
  everything()
)
```

This is a regular tsibble with energy demand `general_supply_kwh` available every 30 minutes from 2012 to 2014. Typically, the first thing we should have at our disposal for examining periodicities of energy behavior across time granularities is to know the number of time granularities we can look at exhaustively. If we consider conventional time deconstructions for a Gregorian calendar, the following time granularities can be considered for analysis. 

```{r search_gran}
smart_meter10 %>% search_gran()
```

The default for search gran in this case, provides temporal granularities ranging from half-hour to year. If these options are considered too many, the default options can be modified to limit the possibilities. For example, the most coarse temporal unit can be set to be a "month".

```{r search_gran_limit}

smart_meter10 %>% 
  search_gran(highest_unit = "month")
```

This looks better. However, some intermediate temporal units might not be pertinent to the analysis and we might want to  remove them from the list of granularities that we want to examine.

```{r search_gran_limit2}

smart_meter10 %>% 
  search_gran(
  highest_unit = "month",
  filter_out = c("hhour", "fortnight")
)
```


### Create any temporal granularity with `create_gran()`

After we have the set of granularities to look at, we should be able to compute any granularity using `create_gran`. These can be be used for exploring distribution of the time series across univariate temporal granularity or computing summary statistics across these categorizations. 


```{r create_gran, echo=TRUE}
smart_meter10 %>%
  create_gran("day_fortnight") %>%
  ggplot2::ggplot(aes(
    x = as.factor(day_fortnight),
    y = general_supply_kwh
  )) +
  scale_y_sqrt() +
  xlab("day_fortnight") +
  geom_lv(
    outlier.colour = "red",
    aes(fill = ..LV..),
    k = 5
  ) +
  scale_color_brewer(palette = "Dark2")


smart_meter10 %>%
  create_gran("day_fortnight") %>%
  group_by(day_fortnight)
```


## Interaction

If we want to look at two of these granularities at a time, it is equivalent to choosing 6 permutation 2 pairs, which essentially means we need to examine 30 plots. However, we need to be cognizant of how these granularities interact. Harmony/clash can be identified to considerably reduce the number of visualizations that can aid exploratory analysis.

```{r is_harmony}
smart_meter10 %>% 
  is_harmony(gran1 = "hour_day", 
             gran2 = "day_week")

smart_meter10 %>%
  is_harmony(gran1 = "hour_day", 
             gran2 = "day_week", 
             facet_h = 14)

smart_meter10 %>% 
  is_harmony(gran1 = "day_month",
             gran2 = "week_month")
```

Let us now look at all the harmonies that we can examine. Fortunately, we are left with only 13 out of 30 visualizations. 

```{r harmony, echo=TRUE}
smart_meter10 %>% harmony(
  ugran = "month",
  filter_out = c("hhour", "fortnight")
)
```

## Visualization

We want to view distribution of the measured variable `general_supply_kwh` across these harmonies through different distribution plots using `prob_plot`. \ref{fig:hd_dm} shows quantile plot of energy consumption across day-month (x-axis) and hour-day (facet). This will help us to see if for each hour of the day, energy consumption changes across different days of the month. From the plot, we see there is not much variation across day-of-month. 
Moving on, if we want to switch the facet and x-axis variable, we simply reverse the order of time granularities. Here, prob_plot would output a ggplot2 object. So we are free to add any element to the default plot. \ref{fig:dm_hd} shows quantile plot of energy consumption across day-month (facet) and hour-day (x-axis). This plot shows the daily pattern of consumption for each day of the month. There is a morning and evening peak in consumption.


```{r hd_dm, echo=TRUE, fig.cap="Quantile plot across hour-day as facets and day-month across x-axis"}
smart_meter10 %>%
  prob_plot("hour_day",
    "day_month",
    response = "general_supply_kwh",
    plot_type = "quantile",
    quantile_prob = c(0.1, 0.25, 0.5, 0.75, 0.9),
    symmetric = TRUE
  ) +
  scale_y_sqrt()
```

```{r dm_hd, echo=TRUE}
smart_meter10 %>%
  prob_plot("day_month",
    "hour_day",
    response = "general_supply_kwh",
    plot_type = "quantile",
    quantile_prob = c(0.1, 0.25, 0.5, 0.75, 0.9),
    symmetric = TRUE
  )
```

\ref{fig:dw_hd} shows letter-value plot of energy consumption across wknd_wday (facet) and hour-day (x-axis).
On weekdays, letter value C (lower tail) from 7 hours to 16 hours is not distinctly visible for most hours on weekdays compared to weekends, signifying there might be less volatility in these hours for weekdays.  Similar observations can be made for letter values C and D which closely follow M, F, E on weekdays, but not on weekends. [Letter value plots]("https://cran.r-project.org/web/packages/lvplot/index.html) are useful for examining the distribution of the tail of the distribution. 


Currently, we can choose from "boxplot", "violin", "ridge", "lv" or "quantile" to visualize distributions.


```{r hd_dw, echo=TRUE}
smart_meter10 %>% prob_plot("wknd_wday",
  "week_month",
  response = "general_supply_kwh",
  plot_type = "violin"
) + scale_y_sqrt()
```

This provides an useful warning suggesting distribution should be computed with caution as number of observations might vary or not be enough for computing distribution plots.

```{r granobs, echo=TRUE}
smart_meter10 %>% gran_obs(
  "week_month",
  "wknd_wday"
)
```

In this case, number of observations are much less on weekends compared to weekdays, but enough to have reliable estimates of distribution.

We can also have recommendations on plots and general check points before plotting two granularities using 
`gran_advice`.

```{r granadvice}
smart_meter10 %>% gran_advice(
  "week_month",
  "wknd_wday"
)
```

### How prob_plot works

Recommendations for distribution plots depend on the levels of the two granularities plotted. They will vary depending on which granularity is placed on the x-axis and which one across facets. Assumptions are made to ensure display is not too cluttered by the space occupied by various kinds of distribution plots. Moreover, the recommendation system ensures that there are just enough observations before choosing a distribution plot. 

`prob_plot` choose the recommended plots only if plot_type = NULL. Warnings are generated if users try to plot clashes, number of facet variables are too high or number of observations used to compute statistical summaries for distribution are not large enough.


## Acknowledgements

Thanks to PhD supervisors [Prof. Rob J Hyndman](https://robjhyndman.com/), [Prof. Dianne Cook](http://dicook.org/) and [Google Summer of Code 2019](https://summerofcode.withgoogle.com/) mentor [Prof. Antony Unwin](http://rosuda.org/~unwin/) for their support and always leading by example. The fine balance of encouraging me to work on my ideas and stepping in to help when I need has made the devlopment of this package a great learning experience for me.

Moreover, I want to thank my cohort at [NUMBATS](https://www.monash.edu/news/articles/team-profile-monash-business-analytics-team) at Monash University for always lending an ear and sharing their wisdom and experience on developing R packages whenever needed. 


